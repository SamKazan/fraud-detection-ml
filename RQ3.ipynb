{"cells":[{"cell_type":"markdown","id":"3dfbf5e9-0e56-4310-86da-980ca9b7c7e8","metadata":{"tags":[],"id":"3dfbf5e9-0e56-4310-86da-980ca9b7c7e8"},"source":["# RQ3: How can clustering techniques be used to identify groups of transactions that may be part of coordinated fraud activities? #"]},{"cell_type":"code","execution_count":null,"id":"a73fb240-f6ea-4841-9264-e8e9615b67ab","metadata":{"tags":[],"id":"a73fb240-f6ea-4841-9264-e8e9615b67ab"},"outputs":[],"source":["import os\n","os.environ['PATH'] += os.pathsep + os.path.expanduser('~/.local/bin')\n","import sys\n","sys.path.append(os.path.expanduser('~/.local/lib/python3.10/site-packages'))"]},{"cell_type":"code","execution_count":null,"id":"44b23608-84c6-45eb-8ea7-55c224ff6add","metadata":{"tags":[],"id":"44b23608-84c6-45eb-8ea7-55c224ff6add"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","from sklearn.metrics import silhouette_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import hdbscan\n","import gc\n","from scipy.sparse import csr_matrix, vstack\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from joblib import Parallel, delayed\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","import random"]},{"cell_type":"markdown","id":"39b32e47-9128-4926-a9e2-eae3ec5a963b","metadata":{"id":"39b32e47-9128-4926-a9e2-eae3ec5a963b"},"source":["## Data Loading and Merging\n","- **Objective**: Load the transaction and identity datasets and merge them based on `TransactionID`.\n","- **Steps**:\n","  - Loaded `train_transaction.csv` and `train_identity.csv`.\n","  - Merged the datasets using an inner join on `TransactionID`."]},{"cell_type":"code","execution_count":null,"id":"4348246f-d573-40fe-82cb-09fbe21523f4","metadata":{"tags":[],"id":"4348246f-d573-40fe-82cb-09fbe21523f4"},"outputs":[],"source":["#Loading the datasets\n","data_transaction = pd.read_csv('train_transaction.csv')\n","data_identity = pd.read_csv('train_identity.csv')"]},{"cell_type":"code","execution_count":null,"id":"ee52d877-ccc3-4275-abcb-b5a2ef49d75b","metadata":{"tags":[],"id":"ee52d877-ccc3-4275-abcb-b5a2ef49d75b"},"outputs":[],"source":["#Merging datasets on TransactionID\n","data = pd.merge(data_transaction, data_identity, on='TransactionID', how='left')"]},{"cell_type":"markdown","id":"ceca2157-544f-45ee-b619-b5e2e0927de4","metadata":{"tags":[],"id":"ceca2157-544f-45ee-b619-b5e2e0927de4"},"source":["## Feature Selection\n","- **Objective**: Select relevant features based on the project plan and methodology.\n","- **Selected Features**:\n","  - Numerical: `TransactionAmt`\n","  - Categorical: `card1`, `card2`, `card3`, `card4`, `card5`, `card6`, `addr1`, `addr2`\n","  - Derived: `D1` to `D15`, `V1` to `V339`"]},{"cell_type":"code","execution_count":null,"id":"ef84953a-a1ab-4393-80e3-7d11ddd2877e","metadata":{"tags":[],"id":"ef84953a-a1ab-4393-80e3-7d11ddd2877e"},"outputs":[],"source":["#Feature selection based on our project plan and methodology\n","selected_features = ['TransactionAmt', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2'] + [f'D{i}' for i in range(1, 16)] + [f'V{i}' for i in range(1, 340)]\n","data = data[selected_features]"]},{"cell_type":"markdown","id":"cc74e429-df35-4cf8-b41a-8f98c72205cd","metadata":{"tags":[],"id":"cc74e429-df35-4cf8-b41a-8f98c72205cd"},"source":["## Optimizing Data Types\n","- **Objective**: Optimize data types to save memory and improve computational efficiency.\n","- **Steps**:\n","  - Converted float64 columns to float32.\n","  - Converted int64 columns to int32."]},{"cell_type":"code","execution_count":null,"id":"f8968369-6236-48af-aff2-dc3bbadaeb03","metadata":{"tags":[],"id":"f8968369-6236-48af-aff2-dc3bbadaeb03"},"outputs":[],"source":["#Optimizing data types to save memory\n","def optimize_data_types(df):\n","    for col in df.select_dtypes(include=['float64']).columns:\n","        df[col] = df[col].astype('float32')\n","    for col in df.select_dtypes(include=['int64']).columns:\n","        df[col] = df[col].astype('int32')\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"ccdd8bf8-d914-4dbb-8585-1c3960e00dba","metadata":{"tags":[],"id":"ccdd8bf8-d914-4dbb-8585-1c3960e00dba"},"outputs":[],"source":["data = optimize_data_types(data)"]},{"cell_type":"markdown","id":"5cfb8ce8-57f3-4381-a210-ceb0fe10e95f","metadata":{"id":"5cfb8ce8-57f3-4381-a210-ceb0fe10e95f"},"source":["## Normalization\n","- **Objective**: Standardize numerical features to have a mean of 0 and a standard deviation of 1.\n","- **Steps**:\n","  - Applied `StandardScaler` to `TransactionAmt`, `addr1`, and `addr2`."]},{"cell_type":"code","execution_count":null,"id":"b6b525f6-ca61-488b-a8bc-b54774d796f8","metadata":{"tags":[],"id":"b6b525f6-ca61-488b-a8bc-b54774d796f8"},"outputs":[],"source":["#Normalizing numerical features\n","scaler = StandardScaler()\n","data[['TransactionAmt', 'addr1', 'addr2']] = scaler.fit_transform(data[['TransactionAmt', 'addr1', 'addr2']])"]},{"cell_type":"markdown","id":"a85ce0a6-ae9c-4cbc-a7f6-c345997b563a","metadata":{"id":"a85ce0a6-ae9c-4cbc-a7f6-c345997b563a"},"source":["## Encoding Categorical Features\n","- **Objective**: Convert categorical variables into a numerical format suitable for machine learning algorithms.\n","- **Steps**:\n","  - Used `OneHotEncoder` to encode `card1`, `card2`, `card3`, `card4`, `card5`, and `card6`.\n","  - Converted the encoded features into a sparse DataFrame for memory efficiency.\n","  - Concatenated the encoded features back with the original DataFrame and dropped the original categorical columns."]},{"cell_type":"code","execution_count":null,"id":"7c2e4120-59d7-4308-ab11-69d516a8fc66","metadata":{"tags":[],"id":"7c2e4120-59d7-4308-ab11-69d516a8fc66"},"outputs":[],"source":["#One-hot encoding categorical features using sparse matrices\n","categorical_cols = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n","encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n","encoded_cols = encoder.fit_transform(data[categorical_cols])"]},{"cell_type":"code","execution_count":null,"id":"5b99c1c3-7c6f-4f24-954c-e413b250d30a","metadata":{"tags":[],"id":"5b99c1c3-7c6f-4f24-954c-e413b250d30a"},"outputs":[],"source":["#Converting to sparse df (more memory efficient)\n","encoded_df = pd.DataFrame.sparse.from_spmatrix(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n","\n","#Concatenating with original df\n","data = pd.concat([data.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n","data.drop(categorical_cols, axis=1, inplace=True)"]},{"cell_type":"markdown","id":"fc68eedc-f6d3-44f6-bf22-99b6b660fea2","metadata":{"id":"fc68eedc-f6d3-44f6-bf22-99b6b660fea2"},"source":["## Imputing Missing Values\n","- **Objective**: Handle missing values in the dataset to ensure complete data for analysis.\n","- **Steps**:\n","  - Implemented a function to impute missing values in chunks to handle large datasets efficiently.\n","  - Used `SimpleImputer` with the mean strategy to fill missing values."]},{"cell_type":"code","execution_count":null,"id":"79f1a475-da79-4fd7-87e7-4d3e698702f5","metadata":{"tags":[],"id":"79f1a475-da79-4fd7-87e7-4d3e698702f5"},"outputs":[],"source":["#Imputing the missing values manually for sparse data\n","def impute_sparse_data(df, chunk_size=10000):\n","    imputer = SimpleImputer(strategy='mean')\n","    sparse_chunks = []\n","\n","    for start in range(0, df.shape[0], chunk_size):\n","        end = min(start + chunk_size, df.shape[0])\n","        chunk = df.iloc[start:end]\n","\n","        #Imputing the chunk\n","        imputed_chunk = imputer.fit_transform(chunk.values)\n","\n","        #Converting back to sparse format\n","        sparse_chunk = csr_matrix(imputed_chunk)\n","        sparse_chunks.append(sparse_chunk)\n","\n","    #Combining all sparse chunks\n","    imputed_sparse_data = vstack(sparse_chunks)\n","    return imputed_sparse_data"]},{"cell_type":"code","execution_count":null,"id":"e7c2f176-3cd2-4bcb-8967-af6f394ff232","metadata":{"tags":[],"id":"e7c2f176-3cd2-4bcb-8967-af6f394ff232"},"outputs":[],"source":["#Using the function to impute the data\n","imputed_data_sparse = impute_sparse_data(data)"]},{"cell_type":"code","execution_count":null,"id":"27e04205-ac22-4ae0-b28d-ba8e6b27a582","metadata":{"tags":[],"id":"27e04205-ac22-4ae0-b28d-ba8e6b27a582"},"outputs":[],"source":["#Converting the imputed sparse matrix back to a df\n","data = pd.DataFrame.sparse.from_spmatrix(imputed_data_sparse, columns=data.columns)\n","\n","#Clearing unused variables to free up memory\n","gc.collect()"]},{"cell_type":"markdown","id":"149bd26c-c625-4fdf-a43b-64246a061064","metadata":{"id":"149bd26c-c625-4fdf-a43b-64246a061064"},"source":["## Dimensionality Reduction\n","- **Objective**: Reduce the dimensionality of the dataset to facilitate clustering.\n","- **Steps**:\n","  - Applied PCA to reduce the dataset to 10 principal components."]},{"cell_type":"code","execution_count":null,"id":"814155ad-14fa-410e-aeda-2c8af5536ea6","metadata":{"tags":[],"id":"814155ad-14fa-410e-aeda-2c8af5536ea6"},"outputs":[],"source":["pca = PCA(n_components=10)\n","data_reduced = pca.fit_transform(data)"]},{"cell_type":"markdown","id":"dcc93c47-9416-427a-8acc-377a3bc24dec","metadata":{"id":"dcc93c47-9416-427a-8acc-377a3bc24dec"},"source":["### K-Means Clustering\n","- **Objective**: Partition the dataset into distinct clusters using K-Means.\n","- **Steps**:\n","  - Applied K-Means clustering with 5 clusters.\n","  - Stored the cluster labels in the DataFrame."]},{"cell_type":"code","execution_count":null,"id":"752471cc-9dc6-4ebd-9cb3-837bea63268e","metadata":{"tags":[],"id":"752471cc-9dc6-4ebd-9cb3-837bea63268e"},"outputs":[],"source":["#K-Means Clustering\n","kmeans = KMeans(n_clusters=5, random_state=42)\n","kmeans_labels = kmeans.fit_predict(data_reduced)\n","data['KMeans_Cluster'] = kmeans_labels"]},{"cell_type":"markdown","id":"31a641c7-3d91-4e65-9abc-0abcfe166442","metadata":{"id":"31a641c7-3d91-4e65-9abc-0abcfe166442"},"source":["### HDBSCAN Clustering\n","- **Objective**: Identify clusters of varying density using HDBSCAN.\n","- **Steps**:\n","  - Applied HDBSCAN with `min_samples` of 10 and `min_cluster_size` of 500.\n","  - Stored the cluster labels in the DataFrame."]},{"cell_type":"code","execution_count":null,"id":"8ba604ee-2bc9-427a-b5af-cb96bc4d3bb1","metadata":{"tags":[],"id":"8ba604ee-2bc9-427a-b5af-cb96bc4d3bb1"},"outputs":[],"source":["#HDBSCAN Clustering\n","clusterer = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500)\n","hdbscan_labels = clusterer.fit_predict(data_reduced)\n","data['HDBSCAN_Cluster'] = hdbscan_labels"]},{"cell_type":"markdown","id":"7a34dc72-76d5-4503-8415-797e23a7da6b","metadata":{"id":"7a34dc72-76d5-4503-8415-797e23a7da6b"},"source":["### Hierarchical Clustering\n","- **Objective**: Understand the hierarchical structure of the data through clustering.\n","- **Steps**:\n","  - Randomly sampled 20,000 data points due to memory constraints.\n","  - Applied Agglomerative Clustering with 5 clusters.\n","  - Stored the cluster labels in the sampled DataFrame."]},{"cell_type":"code","execution_count":null,"id":"dc48a914-5729-4b92-a9b8-0ce88eabc723","metadata":{"tags":[],"id":"dc48a914-5729-4b92-a9b8-0ce88eabc723"},"outputs":[],"source":["#Randomly sampling the data for hierarchical clustering\n","sample_size = 20000\n","sampled_indices = np.random.choice(data_reduced.shape[0], sample_size, replace=False)\n","sampled_data_reduced = data_reduced[sampled_indices]"]},{"cell_type":"code","execution_count":null,"id":"21684494-a52d-449d-a6c7-c9ebe9d719b4","metadata":{"tags":[],"id":"21684494-a52d-449d-a6c7-c9ebe9d719b4"},"outputs":[],"source":["#Hierarchical Clustering on the sampled data\n","hierarchical = AgglomerativeClustering(n_clusters=5)\n","hierarchical_labels = hierarchical.fit_predict(sampled_data_reduced)\n","\n","#Creating a df for the sampled data to store the labels\n","sampled_data_df = pd.DataFrame(sampled_data_reduced, columns=[f'PC{i+1}' for i in range(sampled_data_reduced.shape[1])])\n","sampled_data_df['Hierarchical_Cluster'] = hierarchical_labels"]},{"cell_type":"markdown","id":"18f38308-3ec6-43b4-b6f1-e4e28354ac20","metadata":{"id":"18f38308-3ec6-43b4-b6f1-e4e28354ac20"},"source":["## Evaluation and Visualization\n","\n","### Evaluating Clustering with WCSS\n","- **Objective**: Determine the optimal number of clusters using the Elbow Method.\n","- **Steps**:\n","  - Calculated WCSS for different numbers of clusters.\n","  - Plotted the Elbow Method graph to identify the optimal number of clusters."]},{"cell_type":"code","execution_count":null,"id":"9675cbca-c2d5-4a17-9439-22b84c62e565","metadata":{"tags":[],"id":"9675cbca-c2d5-4a17-9439-22b84c62e565"},"outputs":[],"source":["#Evaluating K-Means with WCSS\n","wcss = []\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters=i, random_state=42)\n","    kmeans.fit(data_reduced)\n","    wcss.append(kmeans.inertia_)\n","plt.plot(range(1, 11), wcss)\n","plt.title('Elbow Method for Optimal K')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()"]},{"cell_type":"markdown","id":"d926aa1b-1918-4b1b-9045-0b64e0f9d7cb","metadata":{"id":"d926aa1b-1918-4b1b-9045-0b64e0f9d7cb"},"source":["### Silhouette Score Calculation\n","- **Objective**: Evaluate the quality of clustering using the Silhouette Score.\n","- **Steps**:\n","  - Randomly sampled 20,000 data points to calculate the Silhouette Score for K-Means, HDBSCAN, and Hierarchical Clustering.\n","  - Reported the Silhouette Scores for each clustering method."]},{"cell_type":"code","execution_count":null,"id":"439bf2af-c7ea-49ad-bdfe-fb8953b879ee","metadata":{"tags":[],"id":"439bf2af-c7ea-49ad-bdfe-fb8953b879ee"},"outputs":[],"source":["#Silhouette Score Sampling\n","sample_size_for_silhouette = 20000\n","random_indices_silhouette = np.random.choice(data_reduced.shape[0], sample_size_for_silhouette, replace=False)\n","sampled_data_reduced_silhouette = data_reduced[random_indices_silhouette]\n","sampled_kmeans_labels = kmeans_labels[random_indices_silhouette]\n","sampled_hdbscan_labels = hdbscan_labels[random_indices_silhouette]"]},{"cell_type":"code","execution_count":null,"id":"5eb44f6c-3a65-4f21-b276-66f072ead1ea","metadata":{"tags":[],"id":"5eb44f6c-3a65-4f21-b276-66f072ead1ea"},"outputs":[],"source":["#Silhouette Score for K-Means\n","kmeans_silhouette = silhouette_score(sampled_data_reduced_silhouette, sampled_kmeans_labels)\n","print(f'K-Means Silhouette Score: {kmeans_silhouette}')\n","\n","#Silhouette Score for HDBSCAN\n","hdbscan_silhouette = silhouette_score(sampled_data_reduced_silhouette, sampled_hdbscan_labels)\n","print(f'HDBSCAN Silhouette Score: {hdbscan_silhouette}')\n","\n","#Silhouette Score for Hierarchical Clustering\n","#Ensuring sampled_data_reduced_silhouette is used instead of sampled_data_reduced for hierarchical clustering\n","sampled_hierarchical_labels = hierarchical_labels[:sample_size_for_silhouette]\n","hierarchical_silhouette = silhouette_score(sampled_data_reduced_silhouette, sampled_hierarchical_labels)\n","print(f'Hierarchical Silhouette Score: {hierarchical_silhouette}')"]},{"cell_type":"code","execution_count":null,"id":"e1fb4666-3886-4f2c-ab42-1558e6ff5e1f","metadata":{"tags":[],"id":"e1fb4666-3886-4f2c-ab42-1558e6ff5e1f"},"outputs":[],"source":["#Silhouette Scores\n","kmeans_silhouette = 0.8564298897506002\n","hdbscan_silhouette = -0.18430016878234834\n","hierarchical_silhouette = -0.0637329143989308\n","\n","#Labels and Scores\n","labels = ['K-Means', 'HDBSCAN', 'Hierarchical']\n","scores = [kmeans_silhouette, hdbscan_silhouette, hierarchical_silhouette]\n","\n","#Plotting the Silhouette Scores\n","plt.figure(figsize=(10, 6))\n","bars = plt.bar(labels, scores, color=['blue', 'green', 'red'])\n","\n","#Adding the scores above the bars\n","for bar in bars:\n","    yval = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, round(yval, 2), ha='center', va='bottom')\n","\n","plt.ylim(-1, 1)\n","plt.axhline(0, color='gray', linewidth=0.8)\n","plt.title('Silhouette Scores for Different Clustering Methods')\n","plt.xlabel('Clustering Method')\n","plt.ylabel('Silhouette Score')\n","plt.show()"]},{"cell_type":"markdown","id":"1ff9d562-1e82-40cb-8380-8ec57b843edf","metadata":{"id":"1ff9d562-1e82-40cb-8380-8ec57b843edf"},"source":["### Visualizing the Clusters\n","\n","#### Scatter Plot for K-Means\n","- **Objective**: Visualize the clusters formed by K-Means.\n","- **Steps**:\n","  - Created a scatter plot for the first two principal components colored by K-Means cluster labels."]},{"cell_type":"code","execution_count":null,"id":"5d468892-cd42-4898-932b-08accfbfe024","metadata":{"tags":[],"id":"5d468892-cd42-4898-932b-08accfbfe024"},"outputs":[],"source":["#Scatter plot for K-Means\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(x=data_reduced[:, 0], y=data_reduced[:, 1], hue=kmeans_labels, palette='viridis')\n","plt.title('K-Means Clustering')\n","plt.show()"]},{"cell_type":"markdown","id":"e52d1007-2a31-40ed-adcf-e167845bb414","metadata":{"id":"e52d1007-2a31-40ed-adcf-e167845bb414"},"source":["#### Heatmap of Selected Features\n","- **Objective**: Visualize the correlations between a subset of selected features to understand the relationships within the data.\n","- **Steps**:\n","  - Selected a subset of features for the correlation matrix to focus on key variables.\n","  - Sampled 20,000 data points to reduce memory usage and ensure computational efficiency.\n","  - Ensured the sampled data is in a dense format for processing.\n","  - Standardized the selected features without centering to prepare for correlation analysis.\n","  - Computed the correlation matrix on the standardized data.\n","  - Plotted a heatmap with adjusted font size and fewer decimal places to visualize the correlations between the selected features.\n","\n","This approach allows us to visually assess the relationships between the selected features, providing insights into potential patterns and interactions within the data."]},{"cell_type":"code","execution_count":null,"id":"bc07864d-509d-432e-aea7-c93afff8e4c8","metadata":{"tags":[],"id":"bc07864d-509d-432e-aea7-c93afff8e4c8"},"outputs":[],"source":["#Selecting a subset of features for the correlation matrix\n","selected_features_subset = ['TransactionAmt', 'card1', 'card2', 'card3', 'addr1', 'addr2'] + [f'D{i}' for i in range(1, 6)] + [f'V{i}' for i in range(1, 21)]\n","\n","#Ensuring the selected features are in the dataset\n","selected_features_subset = [feature for feature in selected_features_subset if feature in data.columns]\n","\n","#Sampling the data to reduce memory usage\n","sample_size = 20000\n","sampled_data_for_corr = data[selected_features_subset].sample(n=sample_size, random_state=42)\n","\n","#Ensuring the sampled data is dense\n","sampled_data_for_corr_dense = sampled_data_for_corr.sparse.to_dense()\n","\n","#Standardizing the data without centering (with_mean=False)\n","scaler = StandardScaler(with_mean=False)\n","sampled_data_for_corr_scaled = pd.DataFrame(scaler.fit_transform(sampled_data_for_corr_dense), columns=selected_features_subset)\n","\n","#Computing the correlation matrix on the standardized data\n","corr_matrix_sampled_scaled = sampled_data_for_corr_scaled.corr()\n","\n","#Plotting the heatmap with adjusted font size and fewer decimal places\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(corr_matrix_sampled_scaled, annot=True, fmt='.1f', annot_kws={\"size\": 8}, cmap='coolwarm')\n","plt.title('Heatmap of Selected Features (Sampled and Standardized Data)')\n","plt.show()"]},{"cell_type":"markdown","id":"4f0eb45f-daea-48d5-ac9c-11a8b376878b","metadata":{"id":"4f0eb45f-daea-48d5-ac9c-11a8b376878b"},"source":["#### Dendrogram for Hierarchical Clustering\n","- **Objective**: Visualize the hierarchical structure of the clusters to understand the clustering relationships within the dataset.\n","- **Steps**:\n","  - Selected a subset of features for the correlation matrix and dendrogram to focus on key variables.\n","  - Sampled 5000 data points to reduce memory usage and ensure computational efficiency.\n","  - Standardized the selected features without centering to prepare for clustering.\n","  - Computed the correlation matrix and plotted a heatmap to visualize correlations between selected features.\n","  - Created a dendrogram using the linkage method with 'ward' to display the hierarchical clustering.\n","  - Increased the figure size and rotated x-axis labels for better readability.\n","  - Optionally truncated the dendrogram to show only the top levels of the hierarchy, making it more readable.\n","\n","This approach allows us to visually assess the hierarchical relationships between clusters and understand how the selected features contribute to the clustering structure."]},{"cell_type":"code","execution_count":null,"id":"df2529a3-7a11-44f4-ad25-8c1fa031cf48","metadata":{"tags":[],"id":"df2529a3-7a11-44f4-ad25-8c1fa031cf48"},"outputs":[],"source":["#Selecting a subset of features for the correlation matrix\n","selected_features_subset = ['TransactionAmt', 'card1', 'card2', 'card3', 'addr1', 'addr2'] + [f'D{i}' for i in range(1, 6)] + [f'V{i}' for i in range(1, 21)]\n","\n","#Ensuring the selected features are in the dataset\n","selected_features_subset = [feature for feature in selected_features_subset if feature in data.columns]\n","\n","#Sampling the data to reduce memory usage\n","sample_size = 5000\n","sampled_data_for_corr = data[selected_features_subset].sample(n=sample_size, random_state=42)\n","\n","#Ensuring the sampled data is dense\n","sampled_data_for_corr_dense = sampled_data_for_corr.sparse.to_dense()\n","\n","#Standardizing the data without centering (with_mean=False)\n","scaler = StandardScaler(with_mean=False)\n","sampled_data_for_corr_scaled = pd.DataFrame(scaler.fit_transform(sampled_data_for_corr_dense), columns=selected_features_subset)\n","\n","#Computing the correlation matrix on the standardized data\n","corr_matrix_sampled_scaled = sampled_data_for_corr_scaled.corr()\n","\n","#Plotting the heatmap with adjusted font size and fewer decimal places\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(corr_matrix_sampled_scaled, annot=True, fmt='.1f', annot_kws={\"size\": 8}, cmap='coolwarm')\n","plt.title('Heatmap of Selected Features (Sampled and Standardized Data)')\n","plt.show()\n","\n","#Dendrogram for Hierarchical Clustering on the sampled data\n","linked = linkage(sampled_data_for_corr_scaled, method='ward')\n","\n","#Plotting the dendrogram with increased figure size and rotated labels\n","plt.figure(figsize=(15, 10))\n","dendrogram(linked)\n","plt.xticks(rotation=90)\n","plt.title('Hierarchical Clustering Dendrogram')\n","plt.show()\n","\n","#Truncating the dendrogram to make it more readable\n","plt.figure(figsize=(15, 10))\n","dendrogram(linked, truncate_mode='level', p=5)\n","plt.xticks(rotation=90)\n","plt.title('Hierarchical Clustering Dendrogram (Truncated)')\n","plt.show()"]},{"cell_type":"markdown","id":"1f4942bc-ecfb-4638-859f-258fee018f15","metadata":{"id":"1f4942bc-ecfb-4638-859f-258fee018f15"},"source":["## Conclusion\n","- **Summary**:\n","  - The analysis provided insights into the clustering structure of the dataset.\n","  - K-Means clustering showed promising results with a high silhouette score.\n","  - Hierarchical clustering and HDBSCAN require further parameter tuning or alternative approaches to improve performance.\n"]},{"cell_type":"code","execution_count":null,"id":"dc8ac5e8-3ccf-4377-86d0-65fedefe49c5","metadata":{"id":"dc8ac5e8-3ccf-4377-86d0-65fedefe49c5"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python (Local)","language":"python","name":"base"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}